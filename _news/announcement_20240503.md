---
layout: post
date: 2024-05-03 08:00:00-0400
inline: true
related_posts: true
---

It turns out the learning rate (step size) chosen in gradient descent can be theoretically shown to exceed $\frac{1}{\lambda_{\max}}$ while still ensuring convergence, as long as it is combined with other steps with some smaller learning rates! Check out our new paper for this interesting result!
