<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://lclewis.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lclewis.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-02T20:26:07+00:00</updated><id>https://lclewis.github.io/feed.xml</id><title type="html">blank</title><subtitle>My personal webpage. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">You know what? The learning rate in gradient descent can go beyond the largest eigenvalue!</title><link href="https://lclewis.github.io/blog/2024/mrgd/" rel="alternate" type="text/html" title="You know what? The learning rate in gradient descent can go beyond the largest eigenvalue!"/><published>2024-05-03T12:00:00+00:00</published><updated>2024-05-03T12:00:00+00:00</updated><id>https://lclewis.github.io/blog/2024/mrgd</id><content type="html" xml:base="https://lclewis.github.io/blog/2024/mrgd/"><![CDATA[<p>When solving a general linear system using gradient descent, a straightforward analysis often suggests that the step size must not exceed \(\frac{1}{\lambda_{\max}}\), where \(\lambda_{\max}\) is the largest eigenvalue of the system. Otherwise, the iteration will diverge.</p> <p>However, when the underlying system exhibits multiscale characteristics, i.e. its eigenvalues cluster into groups with significant differences in magnitude, the conventional wisdom above becomes less reliable. In fact, we show that by employing a combination of large and small learning rates, where the larger rate may exceed \(\frac{1}{\lambda_{\max}}\), the gradient descent can still converge under certain conditions on the number of steps for each rate.</p> <p>This study moves beyond empirical approaches to learning rate selection, offering a more systematic, data-informed strategy to optimize learning rates to improve training efficiency.</p>]]></content><author><name></name></author><category term="paper-posts"/><category term="paper"/><summary type="html"><![CDATA[a short introduction to the MrGD paper]]></summary></entry><entry><title type="html">Attended the 40th ICML 2023, exciting to meet so many new people with many interesting works!</title><link href="https://lclewis.github.io/blog/2023/ICML/" rel="alternate" type="text/html" title="Attended the 40th ICML 2023, exciting to meet so many new people with many interesting works!"/><published>2023-07-25T11:59:00+00:00</published><updated>2023-07-25T11:59:00+00:00</updated><id>https://lclewis.github.io/blog/2023/ICML</id><content type="html" xml:base="https://lclewis.github.io/blog/2023/ICML/"><![CDATA[<p>I presented my work “<a href="https://icml.cc/media/PosterPDFs/ICML%202023/27589.png?t=1691533674.7285285">Linear Regression on Manifold Structured Data</a>” (preview shown below) at the <a href="https://icml.cc/virtual/2023/workshop/21480">2nd Annual Workshop on Topology, Algebra, and Geometry in Machine Learning (TAG-ML)</a> at ICML 2023. :sparkles:</p> <p><img src="https://icml.cc/media/PosterPDFs/ICML%202023/27589.png?t=1691533674.7285285" alt="poster" width="740"/></p> <p></p> ]]></content><author><name></name></author><summary type="html"><![CDATA[I presented my work “Linear Regression on Manifold Structured Data” (preview shown below) at the 2nd Annual Workshop on Topology, Algebra, and Geometry in Machine Learning (TAG-ML) at ICML 2023. :sparkles:]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://lclewis.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://lclewis.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://lclewis.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>